# -*- coding: utf-8 -*-
"""model_training (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxddkeU9p8H211M9Xte_Rbs8MOyrKLp-

importing libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from collections import Counter
from imblearn.combine import SMOTETomek
from imblearn.over_sampling import ADASYN
import warnings
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
warnings.filterwarnings("ignore")
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

"""loading the dataset"""

df=pd.read_csv('final_dataset.csv')

"""# New Section"""

df.drop(['Unnamed: 0'],axis=1,inplace=True)

df.head()

from scipy import stats
df['Liveness']=np.log(df['Liveness'])
df['Speechiness']=stats.boxcox(df['Speechiness'])[0]
df['Acousticness']=stats.boxcox(df['Acousticness'])[0]

"""splitting dataset into features and label"""

x=df.iloc[:,:-1]
y=df.iloc[:,-1]

x.columns

counter=Counter(y)
print('before',counter)
smt=SMOTETomek()
balanced_x,balanced_y=smt.fit_sample(x,y)
counter=Counter(balanced_y)
print('after',counter)

"""splitting data into training and testing sets"""

x_train,x_test,y_train,y_test=train_test_split(balanced_x,balanced_y,test_size=0.2)

"""Scaling the dataset"""

from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
scaler=StandardScaler()
# for i in ['Danceability','Energy','Loudness','Speechiness','Acousticness','Liveness','Valence','Tempo','Genre']:
#   scaler.fit(x_train[i])
#   scaled_x_train[i]=scaler.transform(x_train[[i]])
#   scaled_x_test[i]=scaler.transform(x_test[[i]])

scaled_x_train=scaler.fit_transform(x_train)
scaled_x_test=scaler.transform(x_test)

x_train[0]

"""balancing the dataset using smote hybrid

#Decision Tree
"""

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(max_depth=3)
dt.fit(x_train, y_train)

from sklearn import tree
import matplotlib.pyplot as plt

# fig = plt.figure(figsize=(25,20))
# _ = tree.plot_tree(dt,
#                    feature_names=x.columns,
#                    class_names=['Non Hit', "Hit"],
#                    filled=True)

## Model training by using default max_leaf_nodes=None
Model=DecisionTreeClassifier()
Model.fit(scaled_x_train,y_train)
pred=Model.predict(scaled_x_test)
acc=accuracy_score(y_test,pred)
print(acc)

## Hyper parameter tuning using randomized search cross validation
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV

param_dist={"criterion":["gini","entropy"],"max_depth":[1,2,3,4,5,6,7,8,9,10,11]}
grid=RandomizedSearchCV(DecisionTreeClassifier(),param_distributions=param_dist,cv=5,n_jobs=-1)
grid.fit(scaled_x_train,y_train)

grid.best_estimator_

grid.best_score_

final_model=grid.best_estimator_

y_pred=final_model.predict(scaled_x_test)

print('confusion matrix')
print(confusion_matrix(y_test,y_pred))

202/(106+202)

print('classification report')
print(classification_report(y_test,y_pred))

acc=accuracy_score(y_test,y_pred)
print(acc)

#plotting ROC curve
import matplotlib.pyplot as plt
from sklearn import metrics
metrics.plot_roc_curve(final_model,scaled_x_test,y_test)
plt.show()

from sklearn.externals import joblib
joblib.dump(final_model, 'decision_tree_model.pkl')

scaled_x_train

from sklearn.ensemble import AdaBoostClassifier
boost=AdaBoostClassifier(base_estimator=final_model)
boost.fit(scaled_x_train,y_train)
y_predict=boost.predict(scaled_x_test)

#plotting ROC curve
print('score: ', boost.score)
print()
from sklearn import metrics
metrics.plot_roc_curve(boost,scaled_x_test,y_test)
plt.show()

print('classification report')
print(classification_report(y_test,y_predict))

print(confusion_matrix(y_test,y_predict))



param_dist={'n_estimators':[40,50,60,70,80], 'learning_rate':[0.04,0.03,0.02,0.1],'algorithm':['SAMME', 'SAMME.R']}
grid_1=RandomizedSearchCV(boost,param_distributions=param_dist,cv=5,n_jobs=-1)
grid_1.fit(scaled_x_train,y_train)

#plotting ROC curve
print('score: ', grid_1.best_score_)
print()
print('ROC-AUC curve')
from sklearn import metrics
metrics.plot_roc_curve(boosted_model,scaled_x_test,y_test)
plt.show()



predict=grid_1.predict(scaled_x_test)

accuracy_score(y_test, predict)

print('classification report')
print(classification_report(y,pipeline.predict(x)))

boosted_model=grid_1.best_estimator_

grid_1.best_params_

print('confusion_matrix')
print(confusion_matrix(y_test,predict))

from sklearn.pipeline import Pipeline
pipeline=Pipeline([('scaler', StandardScaler()), ('model', boosted_model)])
pipeline.fit(x_train,y_train)

pipeline.predict(x_test)

print(boosted_model.predict(scaled_x_test))
print(y_test)

joblib.dump(pipeline, 'pipeline.pkl')

from sklearn.externals import joblib
joblib.dump(boosted_model, 'decision_tree_boosted.pkl')

import joblib
model = joblib.load('decision_tree_boosted.pkl') 
print(model)

joblib.dump(scaler, 'scaler.pkl')

"""#LDA"""

lda=LinearDiscriminantAnalysis()
lda.fit(scaled_x_train,y_train)
y_pred=lda.predict(scaled_x_test)
print(accuracy_score(y_test,y_pred))

"""hyper tuning lda"""

parameters={'solver':['svd', 'lsqr', 'eigen'],'shrinkage':['auto','None']}
grid=GridSearchCV(LinearDiscriminantAnalysis(store_covariance=True),param_grid=parameters,cv=5,n_jobs=-1)
grid.fit(scaled_x_train,y_train)

grid.best_params_

grid.best_score_

lda_model=grid.best_estimator_

y_pred=lda.predict(scaled_x_test)

print(classification_report(y_test,y_pred))

print(confusion_matrix(y_test,y_pred))

#Plotting ROC curve for LDA
metrics.plot_roc_curve(lda_model,scaled_x_test,y_test)
plt.show()

from sklearn.externals import joblib
joblib.dump(lda_model, 'LDA.pkl')

df.tail()

l=boosted_model.predict(scaled_x_train)

